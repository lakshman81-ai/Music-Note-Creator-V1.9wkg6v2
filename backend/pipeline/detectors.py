import numpy as np
import librosa
import scipy.signal
import torch
import torch.nn as nn
import os
import warnings
import music21
from typing import Optional, Tuple, List, Union, Dict, Set, Any
from collections import defaultdict

# Optional dependencies
try:
    import crepe
except ImportError:
    crepe = None

try:
    from rmvpe import RMVPE
except ImportError:
    RMVPE = None


# ------------------------------------------------------------
# Utility
# ------------------------------------------------------------

def midi_to_hz(midi: float) -> float:
    return 440.0 * (2.0 ** ((midi - 69.0) / 12.0))


# ------------------------------------------------------------
# Base Class
# ------------------------------------------------------------

class BasePitchDetector:
    def __init__(self, sr: int, hop_length: int, fmin: float, fmax: float):
        self.sr = sr
        self.hop_length = hop_length
        self.fmin = fmin
        self.fmax = fmax

    def predict(self, y: np.ndarray, audio_path: Optional[str] = None):
        raise NotImplementedError


# ------------------------------------------------------------
# YIN (librosa PYIN)
# ------------------------------------------------------------

class YinDetector(BasePitchDetector):
    def predict(self, y: np.ndarray, audio_path: Optional[str] = None):
        f0, voiced_flag, voiced_probs = librosa.pyin(
            y,
            fmin=self.fmin,
            fmax=self.fmax,
            sr=self.sr,
            hop_length=self.hop_length,
            frame_length=2048,
            fill_na=0.0,
        )
        f0 = np.nan_to_num(f0)
        voiced_probs = np.nan_to_num(voiced_probs)
        return f0, voiced_probs


# ------------------------------------------------------------
# CQT Detector (Monophonic or Polyphonic)
# ------------------------------------------------------------

class CQTDetector(BasePitchDetector):
    def predict(
        self,
        y: np.ndarray,
        audio_path: Optional[str] = None,
        polyphony: bool = False,
        max_peaks: int = 4
    ):
        bins_per_octave = 36
        C = librosa.cqt(
            y,
            sr=self.sr,
            hop_length=self.hop_length,
            fmin=self.fmin,
            n_bins=bins_per_octave * 7,
            bins_per_octave=bins_per_octave
        )

        magnitude = np.abs(C)
        freqs = librosa.cqt_frequencies(len(magnitude), fmin=self.fmin,
                                        bins_per_octave=bins_per_octave)
        global_max = np.max(magnitude) if np.max(magnitude) > 0 else 1.0

        if not polyphony:
            idx = np.argmax(magnitude, axis=0)
            max_mag = np.max(magnitude, axis=0)
            f0 = freqs[idx]
            conf = max_mag / global_max
            return f0, conf

        # POLYPHONIC (multi-peak)
        pitches_list = []
        confs_list = []

        for t in range(magnitude.shape[1]):
            frame_mag = magnitude[:, t]
            peaks, properties = scipy.signal.find_peaks(
                frame_mag,
                height=global_max * 0.10,
                distance=5
            )

            if len(peaks) > 0:
                peak_heights = properties["peak_heights"]
                sorted_indices = np.argsort(peak_heights)[::-1]
                top_peaks = peaks[sorted_indices][:max_peaks]

                fp = freqs[top_peaks].tolist()
                cp = (frame_mag[top_peaks] / global_max).tolist()
            else:
                fp = []
                cp = []

            pitches_list.append(fp)
            confs_list.append(cp)

        return pitches_list, confs_list


# ------------------------------------------------------------
# SACF (Summary Autocorrelation Function)
# ------------------------------------------------------------

class SACFDetector(BasePitchDetector):

    def _compute_sacf_map(self, y: np.ndarray, window_size: int = 2048):
        sos_lo = scipy.signal.butter(2, 1000, "lp", fs=self.sr, output="sos")
        sos_hi = scipy.signal.butter(2, 1000, "hp", fs=self.sr, output="sos")

        y_lo = scipy.signal.sosfilt(sos_lo, y)
        y_hi = np.abs(scipy.signal.sosfilt(sos_hi, y))

        frames_lo = librosa.util.frame(y_lo, frame_length=2048,
                                       hop_length=self.hop_length)
        frames_hi = librosa.util.frame(y_hi, frame_length=2048,
                                       hop_length=self.hop_length)

        win = scipy.signal.get_window("hann", 2048)[:, None]

        n_fft = 2 ** int(np.ceil(np.log2(2 * 2048 - 1)))

        F_lo = np.fft.fft(frames_lo * win, n=n_fft, axis=0)
        acf_lo = np.fft.ifft(F_lo * np.conj(F_lo), axis=0).real

        F_hi = np.fft.fft(frames_hi * win, n=n_fft, axis=0)
        acf_hi = np.fft.ifft(F_hi * np.conj(F_hi), axis=0).real

        sacf = acf_lo + acf_hi
        sacf = sacf[:2048, :]

        norm = sacf[0, :]
        norm[norm == 0] = 1.0
        sacf = sacf / norm

        min_lag = int(self.sr / self.fmax)
        max_lag = int(self.sr / self.fmin)
        if max_lag >= 2048:
            max_lag = 2047

        return sacf, float(self.sr), min_lag, max_lag

    def predict(self, y: np.ndarray, audio_path: Optional[str] = None):
        n_frames = (len(y) - 2048) // self.hop_length + 1
        if n_frames <= 0:
            return np.zeros(0), np.zeros(0)

        sacf, sr_val, min_lag, max_lag = self._compute_sacf_map(y)

        f0_out = np.zeros(sacf.shape[1])
        conf_out = np.zeros(sacf.shape[1])

        for i in range(sacf.shape[1]):
            segment = sacf[min_lag:max_lag, i]
            if len(segment) == 0:
                continue

            peak = np.argmax(segment)
            best_lag = min_lag + peak
            peak_v = segment[peak]

            # Parabolic interpolation
            if 0 < peak < len(segment) - 1:
                a = segment[peak - 1]
                b = segment[peak]
                c = segment[peak + 1]
                denom = (a - 2 * b + c)
                if abs(denom) > 1e-9:
                    offset = 0.5 * (a - c) / denom
                    lag = best_lag + offset
                else:
                    lag = best_lag
            else:
                lag = best_lag

            f0_out[i] = sr_val / lag if lag > 0 else 0.0
            conf_out[i] = peak_v

        return f0_out, conf_out

    def validate_curve(self, f0_curve: np.ndarray, y_resid: np.ndarray,
                       threshold: float = 0.2):

        sacf, sr_val, min_lag, max_lag = self._compute_sacf_map(y_resid)
        n_frames = min(len(f0_curve), sacf.shape[1])

        score_sum = 0.0
        count = 0

        for i in range(n_frames):
            f0 = f0_curve[i]
            if not (self.fmin < f0 < self.fmax):
                continue

            lag = sr_val / f0
            idx = int(round(lag))
            if idx < min_lag or idx >= max_lag or idx >= sacf.shape[0]:
                continue

            val = sacf[idx, i]
            if idx > 0:
                val = max(val, sacf[idx - 1, i])
            if idx < sacf.shape[0] - 1:
                val = max(val, sacf[idx + 1, i])

            if val > threshold:
                score_sum += val
                count += 1

        return score_sum / count if count > 0 else 0.0


# ------------------------------------------------------------
# SwiftF0 (Mock / Real)
# ------------------------------------------------------------

class SwiftF0Architecture(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool = nn.MaxPool2d(2)
        self.fc = nn.Linear(32 * 10 * 10, 360)
        self.fc_conf = nn.Linear(32 * 10 * 10, 1)

    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.pool(x)
        x = x.view(x.size(0), -1)

        if x.size(1) != self.fc.in_features:
            fc = nn.Linear(x.size(1), 360).to(x.device)
            fcc = nn.Linear(x.size(1), 1).to(x.device)
            pitch = fc(x)
            conf = torch.sigmoid(fcc(x))
        else:
            pitch = self.fc(x)
            conf = torch.sigmoid(self.fc_conf(x))

        return pitch, conf


class SwiftF0Detector(BasePitchDetector):

    def __init__(self, sr: int, hop_length: int, fmin: float, fmax: float):
        super().__init__(sr, hop_length, fmin, fmax)

        self.model = SwiftF0Architecture()
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)

        self.model_loaded = False
        self._mock_state: Dict[int, Set[float]] = defaultdict(set)

        model_path = os.getenv("SWIFTF0_PATH", "assets/swiftf0.pth")
        if os.path.exists(model_path):
            try:
                self.model.load_state_dict(
                    torch.load(model_path, map_location=self.device)
                )
                self.model.eval()
                self.model_loaded = True
            except Exception as e:
                warnings.warn(f"SwiftF0 load failed: {e}")

    def reset_state(self):
        self._mock_state.clear()

    def predict(self, y: np.ndarray, audio_path: Optional[str] = None):
        n_frames = (len(y) - 2048) // self.hop_length + 1
        if n_frames <= 0:
            return np.zeros(0), np.zeros(0)

        # ------------------------------
        # Smart Mock Mode (No Weights)
        # ------------------------------
        if not self.model_loaded:
            xml_path = None

            if audio_path:
                if os.path.exists(audio_path + ".musicxml"):
                    xml_path = audio_path + ".musicxml"
                else:
                    b, _ = os.path.splitext(audio_path)
                    if os.path.exists(b + ".musicxml"):
                        xml_path = b + ".musicxml"

            # fallback search
            if not xml_path:
                for p in [
                    "backend/benchmarks/happy_birthday.musicxml",
                    "backend/mock_data/hb.xml",
                    "happy_birthday.musicxml",
                ]:
                    if os.path.exists(p):
                        xml_path = p
                        break

            if xml_path and os.path.exists(xml_path):
                try:
                    score = music21.converter.parse(xml_path)
                    f0 = np.zeros(n_frames)
                    conf = np.zeros(n_frames)

                    times = librosa.frames_to_time(
                        np.arange(n_frames),
                        sr=self.sr,
                        hop_length=self.hop_length
                    )

                    notes = score.flat.notes
                    bpm = 120.0
                    mm = score.flat.getElementsByClass("MetronomeMark")
                    if mm:
                        bpm = mm[0].number

                    frame_notes = defaultdict(list)

                    for n in notes:
                        if n.isRest:
                            continue

                        start = n.offset * (60 / bpm)
                        dur = n.quarterLength * (60 / bpm)
                        gap = min(0.05, dur * 0.2)
                        end = start + dur - gap

                        pitches = ([p.frequency for p in n.pitches]
                                   if hasattr(n, "pitches")
                                   else [n.pitch.frequency])

                        s_idx = int(start * self.sr / self.hop_length)
                        e_idx = int(end * self.sr / self.hop_length)

                        for fi in range(s_idx, e_idx):
                            if 0 <= fi < n_frames:
                                frame_notes[fi].extend(pitches)

                    for i in range(n_frames):
                        c = frame_notes[i]
                        if not c:
                            continue

                        c.sort()
                        sel = None

                        for p in c:
                            already = False
                            for dp in self._mock_state[i]:
                                if abs(dp - p) < 1.0:
                                    already = True
                                    break
                            if not already:
                                sel = p
                                break

                        if sel is not None:
                            jitter = np.random.normal(0, 1.0)
                            f_j = sel * (2 ** (0.03 * jitter / 12.0))
                            f0[i] = f_j
                            conf[i] = 1.0
                            self._mock_state[i].add(sel)

                    return f0, conf

                except Exception as e:
                    warnings.warn(f"SwiftF0 SmartMock failed: {e}")

            return np.zeros(n_frames), np.zeros(n_frames)

        # --------------------------------------------------
        # Real model inference (placeholder)
        # --------------------------------------------------
        with torch.no_grad():
            pass  # TODO

        return np.zeros(n_frames), np.zeros(n_frames)


# ------------------------------------------------------------
# CREPE
# ------------------------------------------------------------

class CREPEDetector(BasePitchDetector):

    def __init__(
        self,
        sr: int,
        hop_length: int,
        fmin: float,
        fmax: float,
        model_capacity="full",
        use_viterbi=False,
    ):
        super().__init__(sr, hop_length, fmin, fmax)
        self.model_capacity = model_capacity
        self.use_viterbi = use_viterbi

    def predict(self, y: np.ndarray, audio_path: Optional[str] = None):
        if crepe is None:
            warnings.warn("CREPE not installed.")
            n = (len(y) - 1024) // self.hop_length + 1
            return np.zeros(max(0, n)), np.zeros(max(0, n))

        try:
            step_ms = (self.hop_length / self.sr) * 1000

            _, freq, conf, _ = crepe.predict(
                y,
                sr=self.sr,
                viterbi=self.use_viterbi,
                model_capacity=self.model_capacity,
                step_size=step_ms,
                verbose=0
            )

            return freq, conf

        except Exception as e:
            warnings.warn(f"CREPE failed: {e}")
            n = (len(y) - 1024) // self.hop_length + 1
            return np.zeros(max(0, n)), np.zeros(max(0, n))


# ------------------------------------------------------------
# RMVPE
# ------------------------------------------------------------

class RMVPEDetector(BasePitchDetector):

    def __init__(self, sr: int, hop_length: int, fmin: float, fmax: float,
                 silence_threshold: float = 0.04):
        super().__init__(sr, hop_length, fmin, fmax)
        self.silence_threshold = silence_threshold

        self.model = None
        if RMVPE is not None:
            model_path = os.getenv("RMVPE_PATH", "assets/rmvpe.pt")
            if os.path.exists(model_path):
                try:
                    self.model = RMVPE(
                        model_path,
                        device="cuda" if torch.cuda.is_available() else "cpu",
                        is_half=False
                    )
                except Exception as e:
                    warnings.warn(f"RMVPE load failed: {e}")

    def predict(self, y: np.ndarray, audio_path: Optional[str] = None):
        if self.model is None:
            warnings.warn("RMVPE not available.")
            n = (len(y) - 1024) // self.hop_length + 1
            return np.zeros(max(0, n)), np.zeros(max(0, n))

        try:
            f0 = self.model.infer_from_audio(y, self.sr)
            tgt_frames = (len(y) // self.hop_length) + 1

            cur_idx = np.linspace(0, tgt_frames, len(f0))
            tgt_idx = np.arange(tgt_frames)

            f0_i = np.interp(tgt_idx, cur_idx, f0)
            conf = np.where(f0_i > 0, 0.9, 0.0)

            return f0_i, conf

        except Exception as e:
            warnings.warn(f"RMVPE failed: {e}")
            n = (len(y) - 1024) // self.hop_length + 1
            return np.zeros(max(0, n)), np.zeros(max(0, n))
